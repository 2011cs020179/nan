<!DOCTYPE html>
<html>
<head>
  <style>
    /* Add styling for the dark background */
    body {
      background-color: #000;
      color: #fff;
      text-align: center;
    }
    /* Style for the code snippets */
    .code-snippet {
      background-color: #000;
      padding: 10px;
      margin-bottom: 20px;
      display: none;
      text-align: left;
    }
    /* Style for the "Show Code" buttons */
    .show-button {
      background-color: #22222200;
      color: #fff;
      padding: 5px 10px;
      border: none;
      border-radius: 15px;
      cursor: pointer;
      margin-bottom: 10px;
    }
    /* Style for the "Copy Code" buttons */
    .copy-button {
      background-color: #333;
      color: #fff;
      padding: 5px 10px;
      border: none;
      border-radius: 20px;
      cursor: pointer;
      float: right;
    }
  </style>
</head>
<body>
  <div>
  <button class="show-button" onclick="showCode('code-snippet-1')">W1</button>
  <button class="show-button" onclick="showCode('code-snippet-2')">W2 FFN</button>
  <button class="show-button" onclick="showCode('code-snippet-3')">W3 Backpropagation</button>
  <button class="show-button" onclick="showCode('code-snippet-4')">W4 SGD</button>
  <button class="show-button" onclick="showCode('code-snippet-5')">W4 BGD</button>
  <button class="show-button" onclick="showCode('code-snippet-6')">W4 MBGD</button>
  <button class="show-button" onclick="showCode('code-snippet-7')">W5 PCA</button>
  <button class="show-button" onclick="showCode('code-snippet-8')">W6 SVD</button></div>
  <div>
  <button class="show-button" onclick="showCode('code-snippet-9')">W7 RNN</button>
  <button class="show-button" onclick="showCode('code-snippet-10')">W8 CNN</button>
  <button class="show-button" onclick="showCode('code-snippet-11')">W9 AutoEncoders</button>
  <button class="show-button" onclick="showCode('code-snippet-12')">W10 LSTM</button>
  <button class="show-button" onclick="showCode('code-snippet-13')">W11 GAN</button>
  <button class="show-button" onclick="showCode('code-snippet-14')">W12 word2vec</button></div>
  <div class="code-snippet" id="code-snippet-1">
    <button class="copy-button" onclick="copyToClipboard('code-snippet-1')">Copy Code</button>
    <pre>
      <code id="code-snippet-1">
import tensorflow as tf
mnist = tf.keras.datasets.mnist
(x_train,y_train), (x_test,y_test) = mnist.load_data()
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),
                                   tf.keras.layers.Dense(128,activation='relu'),
                                   tf.keras.layers.Dropout(0.2),
                                   tf.keras.layers.Dense(10)])
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer='adam',loss=loss_fn,metrics=['accuracy'])
model.fit(x_train,y_train,epochs=5)
model.evaluate(x_test,y_test,verbose=2)
      </code>
    </pre>
  </div>
  <div class="code-snippet" id="code-snippet-2">
    <button class="copy-button" onclick="copyToClipboard('code-snippet-2')">Copy Code</button>
    <pre>
      <code id="code-snippet-2">
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.colors
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
from tqdm import tqdm_notebook

from sklearn.preprocessing import OneHotEncoder
from sklearn.datasets import make_blobs
class SigmoidNeuron:
  #A class for sigmoid neuron
  
  def __init__(self):
    self.w = None
    self.b = None
    
 
  def perceptron(self, x):
    return np.dot(x, self.w.T) + self.b
  
  def sigmoid(self, x):
    return 1.0/(1.0 + np.exp(-x))
  
  def grad_w_mse(self, x, y):
    y_pred = self.sigmoid(self.perceptron(x))
    return (y_pred - y) * y_pred * (1 - y_pred) * x
  
  def grad_b_mse(self, x, y):
    y_pred = self.sigmoid(self.perceptron(x))
    return (y_pred - y) * y_pred * (1 - y_pred)
  
  def grad_w_ce(self, x, y):
    y_pred = self.sigmoid(self.perceptron(x))
    if y == 0:
      return y_pred * x
    elif y == 1:
      return -1 * (1 - y_pred) * x
    else:
      raise ValueError("y should be 0 or 1")
    
  def grad_b_ce(self, x, y):
    y_pred = self.sigmoid(self.perceptron(x))
    if y == 0:
      return y_pred 
    elif y == 1:
      return -1 * (1 - y_pred)
    else:
      raise ValueError("y should be 0 or 1")
  
  def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, loss_fn="mse", display_loss=False):
    
    # initialise w, b
    if initialise:
      self.w = np.random.randn(1, X.shape[1])
      self.b = 0
      
    if display_loss:
      loss = {}
    
    for i in tqdm_notebook(range(epochs), total=epochs, unit="epoch"):
      dw = 0
      db = 0
      for x, y in zip(X, Y):
        if loss_fn == "mse":
          dw += self.grad_w_mse(x, y)
          db += self.grad_b_mse(x, y) 
        elif loss_fn == "ce":
          dw += self.grad_w_ce(x, y)
          db += self.grad_b_ce(x, y)
          
      m = X.shape[1]    
      self.w -= learning_rate * dw/m
      self.b -= learning_rate * db/m
      
      if display_loss:
        Y_pred = self.sigmoid(self.perceptron(X))
        if loss_fn == "mse":
          loss[i] = mean_squared_error(Y, Y_pred)
        elif loss_fn == "ce":
          loss[i] = log_loss(Y, Y_pred)
    
    if display_loss:
      plt.plot(loss.values())
      plt.xlabel('Epochs')
      if loss_fn == "mse":
        plt.ylabel('Mean Squared Error')
      elif loss_fn == "ce":
        plt.ylabel('Log Loss')
      plt.show()
      
  def predict(self, X):
    Y_pred = []
    for x in X:
      y_pred = self.sigmoid(self.perceptron(x))
      Y_pred.append(y_pred)
    return np.array(Y_pred)
my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list("", ["red", "yellow", "green"])

data, labels = make_blobs(n_samples=1000, centers=4,n_features=2, random_state=0)
print(data.shape, labels.shape)

plt.scatter(data[:,0], data[:,1], c = labels, cmap = my_cmap)
plt.show()
      </code>
    </pre>
  </div>

  <div class="code-snippet" id="code-snippet-3">
    <button class="copy-button" onclick="copyToClipboard('code-snippet-3')">Copy Code</button>
    <pre>
      <code id="code-snippet-3">
#w3 Backpropagation
import numpy as np

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
  return x * (1 - x)

input_size = 2
hidden_size = 3
output_size = 1

weights_hidden = np.random.uniform(size=(input_size, hidden_size))
weights_output = np.random.uniform(size=(hidden_size, output_size))

learning_rate = 0.1
epochs = 2000
X = np.array([[0.5, 0.6], [0, 1], [1, 0], [1, 1]])
y = np.array([[0.7], [1], [1], [0.7]])
for i in range(epochs):
  # Forward pass
  hidden_layer = sigmoid(np.dot(X, weights_hidden))
  output_layer = sigmoid(np.dot(hidden_layer, weights_output))
  error = y - output_layer
  mse = np.mean(np.square(error))
  if i % 100 == 0:
    print(f"Epoch {i}, MSE: {mse}")
    print("Output:", output_layer.T)

  # Backward pass
  output_delta = error * sigmoid_derivative(output_layer)
  hidden_error = output_delta.dot(weights_output.T)
  hidden_delta = hidden_error * sigmoid_derivative(hidden_layer)
  weights_output += learning_rate * hidden_layer.T.dot(output_delta)
  weights_hidden += learning_rate * X.T.dot(hidden_delta)
  
hidden_layer = sigmoid(np.dot(X, weights_hidden))
output_layer = sigmoid(np.dot(hidden_layer, weights_output))
print("Final output:", output_layer.T)
      </code>
    </pre>
  </div>

  <div class="code-snippet" id="code-snippet-4">
    <button class="copy-button" onclick="copyToClipboard('code-snippet-4')">Copy Code</button>
    <pre>
      <code id="code-snippet-4">
#sgd w4
import numpy as np
from matplotlib import pyplot as plt

def compute_cost(X, y, theta):
    m = y.size
    h = X.dot(theta)
    J = (1 / (2 * m)) * np.sum(np.square(h - y))
    return J

def stoch(X, y, alpha, num_iter):
    m = y.size
    n = X.shape[1]
    theta = np.zeros(n)
    j_history = np.zeros(num_iter)

    for i in range(num_iter):
        indices = np.random.permutation(m)
        X = X[indices, :]
        y = y[indices]

        for j in range(m):
            h = X[j, :].dot(theta)
            theta = theta - alpha * (h - y[j]) * X[j, :]
        j_history[i] = compute_cost(X, y, theta)

    return theta, j_history

X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
y = np.array([1, 2, 3, 4, 5])
X = np.hstack([np.ones_like(X), X])
theta, j_history = stoch(X, y, alpha=0.01, num_iter=1000)

plt.plot(j_history)
plt.xlabel("iterations")
plt.ylabel("cost")
plt.show()
      </code>
    </pre>
  </div>

  <div class="code-snippet" id="code-snippet-5">
    <button class="copy-button" onclick="copyToClipboard('code-snippet-5')">Copy Code</button>
    <pre>
      <code id="code-snippet-5">
#batch gradient descent w4
import numpy as np
import matplotlib.pyplot as plt

def batch_gradient_descent(X, y_true, epochs, learning_rate=0.01):
    number_of_features = X.shape[1]
    w = np.ones(shape=(number_of_features))
    b = 0
    total_samples = X.shape[0]
    cost_list = []
    epoch_list = []

    for i in range(epochs):
        y_predicted = np.dot(w, X.T) + b
        w_grad = -(2/total_samples)*(X.T.dot(y_true-y_predicted))
        b_grad = -(2/total_samples)*np.sum(y_true-y_predicted)
        w = w - learning_rate * w_grad
        b = b - learning_rate * b_grad
        cost = np.mean(np.square(y_true-y_predicted))

        if i % 10 == 0:
            cost_list.append(cost)
            epoch_list.append(i)

    return w, b, cost, cost_list, epoch_list

scaled_X = np.array([[0.55, 0.7], [0.62, 0.45], [0.28, 0.9], [0.34, 0.3], [0.12, 0.5], [0.3, 0.2], [0.75, 0.5], [0.73, 0.8]])
scaled_y = np.array([2.45, 2.2, 1.85, 1.45, 1.0, 1.0, 2.0, 2.3])

w, b, cost, cost_list, epoch_list = batch_gradient_descent(scaled_X, scaled_y.reshape(scaled_y.shape[0],), 500)

plt.xlabel("Epoch")
plt.ylabel("Cost")
plt.plot(epoch_list, cost_list)
#plt.title("Batch Gradient Descent")
plt.show()
</code>
</pre>
</div>

<div class="code-snippet" id="code-snippet-6">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-6')">Copy Code</button>
  <pre>
    <code id="code-snippet-6">
#w4 mbgd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

def mini_batch_gradient_descent(X, y, epochs=100, batch_size=5, learning_rate=0.01):
    # Initialize weights and bias
    w = np.ones(X.shape[1])
    b = 0
    
    # Calculate number of batches
    num_batches = len(X) // batch_size
    costs = []
    
    # Loop through epochs
    for i in range(epochs):
        X, y = shuffle(X, y)
        
        for j in range(num_batches):
            # Get batch of X and y
            X_batch = X[j*batch_size:(j+1)*batch_size]
            y_batch = y[j*batch_size:(j+1)*batch_size]
            
            # Make predictions and calculate gradients
            y_pred = X_batch @ w + b
            w_grad = -(2/len(X_batch)) * X_batch.T @ (y_batch - y_pred)
            b_grad = -(2/len(X_batch)) * np.sum(y_batch - y_pred)
            
            # Update weights and bias
            w -= learning_rate * w_grad
            b -= learning_rate * b_grad
            
            # Calculate cost and append to list
            cost = np.mean(np.square(y_batch - y_pred))
            costs.append(cost)
    
    return w, b, costs

# Test the function with example data
X = np.array([[1, 2], [2, 4], [3, 6], [4, 8]])
y = np.array([2, 4, 6, 8])

w, b, costs = mini_batch_gradient_descent(X, y, epochs=100, batch_size=2, learning_rate=0.01)

#Ploting the cost over epochs
plt.xlabel("epoch")
plt.ylabel("cost")
plt.plot(range(len(costs)), costs)
plt.show()
    </code>
  </pre>
</div>

<div class="code-snippet" id="code-snippet-7">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-7')">Copy Code</button>
  <pre>
    <code id="code-snippet-7">
#W5 pca
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
dataset = pd.read_csv(url, names=names)
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
cov_matrix = np.cov(X.T)
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
idx = eigenvalues.argsort()[::-1]
eigenvectors = eigenvectors[:, idx]
cumulative_variance_ratio = np.cumsum(eigenvalues / np.sum(eigenvalues))
plt.plot(cumulative_variance_ratio)
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance ratio')
plt.show()
num_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1
print(f'Number of components chosen: {num_components}')
X_transformed = np.dot(X, eigenvectors[:, :num_components])
for class_name, class_marker in zip(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], ['o', 's', '^']):
    plt.scatter(X_transformed[y == class_name, 0], X_transformed[y == class_name, 1], marker=class_marker, label=class_name)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()
    </code>
  </pre>
</div>

<div class="code-snippet" id="code-snippet-8">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-8')">Copy Code</button>
  <pre>
    <code id="code-snippet-8">
#w6 svd
import numpy as np
A=np.array([[1,2],[3,4],[5,6]])
U,s,Vt=np.linalg.svd(A)
sigma=np.zeros_like(A)
sigma[:min(A.shape),:min(A.shape)]=np.diag(s)
B=np.dot(U,np.dot(sigma,Vt))
print("Original Matrix:\n",A)
print("U:\n",U)
print("s:\n",s)
print("Vt:\n",Vt)
print("Reconstructed Matrix:\n",B)
    </code>
  </pre>
</div>
<div class="code-snippet" id="code-snippet-9">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-9')">Copy Code</button>
  <pre>
    <code id="code-snippet-9">
#w7 RNN
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN

t = np.linspace(0, 160, num=1000)
x = np.sin(t) + np.random.normal(0, 0.1, size=1000)
window_size = 10
X = [x[i:i + window_size] for i in range(len(x) - window_size)]
y = [x[i + window_size] for i in range(len(x) - window_size)]

X = np.array(X)[:, :, np.newaxis]
y = np.array(y)[:, np.newaxis]

X_train, X_test, y_train, y_test = X[:800], X[800:], y[:800], y[800:]

model = Sequential()
model.add(SimpleRNN(32, input_shape=(window_size, 1)))
model.add(Dense(1))
model.compile(loss="mse", optimizer="adam")
model.fit(X_train, y_train, epochs=50, batch_size=32)

y_pred = model.predict(X_test)

plt.plot(t[800 + window_size:], y_pred, label='Predicted')
plt.plot(t[800 + window_size:], y_test, label="True")
plt.legend()
plt.show()
    </code>
  </pre>
</div>
<div class="code-snippet" id="code-snippet-10">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-10')">Copy Code</button>
  <pre>
    <code id="code-snippet-10">
#W8 CNN
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Load the MNIST dataset and preprocess the data
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0
X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))
X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Build the CNN model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Compile and train the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print('Test loss:', loss)
print('Test accuracy:', accuracy)
    </code>
  </pre>
</div>
<div class="code-snippet" id="code-snippet-11">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-11')">Copy Code</button>
  <pre>
    <code id="code-snippet-11">
#W9 AutoEncoder
import numpy as np
n_samples, n_features = 1000, 10
X = np.random.randn(n_samples, n_features)
n_hidden_units = 5
W1 = np.random.randn(n_features, n_hidden_units)
W2 = np.random.randn(n_hidden_units, n_features)
b1 = np.zeros(n_hidden_units)
b2 = np.zeros(n_features)
learning_rate = 0.01
num_epochs = 100
for epoch in range(num_epochs):
    # Forward pass
    hidden = np.maximum(np.dot(X, W1) + b1, 0)
    output = np.dot(hidden, W2) + b2
    loss = np.mean((X - output) ** 2)

    # Backward pass
    output_delta = (output - X) / n_samples
    hidden_delta = np.dot(output_delta, W2.T)
    hidden_delta[hidden < 0] = 0
    W2 -= learning_rate * np.dot(hidden.T, output_delta)
    b2 -= learning_rate * np.sum(output_delta, axis=0)
    W1 -= learning_rate * np.dot(X.T, hidden_delta)
    b1 -= learning_rate * np.sum(hidden_delta, axis=0)
    print(f"Epoch {epoch+1}/{num_epochs}, loss={loss:.4f}")
encoded = np.maximum(np.dot(X, W1) + b1, 0)
decoded = np.dot(encoded, W2) + b2
print("Original data:")
print(X[:5])
print("\nReconstructed data:")
print(decoded[:5])
    </code>
  </pre>
</div>
<div class="code-snippet" id="code-snippet-12">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-12')">Copy Code</button>
  <pre>
    <code id="code-snippet-12">
#w10 lstm
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
max_features = 5000
max_length = 500
batch_size = 32
embedding_dims = 50
hidden_dims = 100
epochs = 5

#Loading the IMDB dataset
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
x_train = sequence.pad_sequences(x_train, maxlen=max_length)
x_test = sequence.pad_sequences(x_test, maxlen=max_length)
model = Sequential()
model.add(Embedding(max_features, embedding_dims, input_length=max_length))
model.add(LSTM(hidden_dims))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))

plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()

loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test loss: {loss}')
print(f'Test accuracy: {accuracy}')
    </code>
  </pre>
</div>
<div class="code-snippet" id="code-snippet-13">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-13')">Copy Code</button>
  <pre>
    <code id="code-snippet-13">
#W11 GAN
import tensorflow.keras
from tensorflow.keras import layers
from keras.datasets import mnist
import numpy as np
import matplotlib.pyplot as plt
encoding_dim = 32
input_img = keras.Input(shape=(784,))
encoded = layers.Dense(encoding_dim, activation='relu')(input_img)
decoded = layers.Dense(784, activation='sigmoid')(encoded)
autoencoder = keras.Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
(x_train, _), (x_test, _) = mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))
encoded_imgs = autoencoder.predict(x_test)
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(encoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
    </code>
  </pre>
</div>
<div class="code-snippet" id="code-snippet-14">
  <button class="copy-button" onclick="copyToClipboard('code-snippet-14')">Copy Code</button>
  <pre>
    <code id="code-snippet-14">
#W12 W2V
from gensim.models import Word2Vec
dataset = [
    ['k', 'am', 'learning'],
    ['I', 'enjoy', 'deep', 'learning'],
    ['I', 'enjoy', 'NLP']
]
model = Word2Vec(dataset, min_count=1)
word = 'learning'
similar_words = model.wv.most_similar(word)
print(f"Words similar to '{word}':")
for similar_word, similarity in similar_words:
    print(f"{similar_word}, Similarity: {similarity:.4f}")
    </code>
  </pre>
</div>
  <script>
    function showCode(elementId) {
      // Get all the code snippets
      var codeSnippets = document.getElementsByClassName("code-snippet");

      // Loop through all the code snippets and hide them
      for (var i = 0; i < codeSnippets.length; i++) {
        codeSnippets[i].style.display = "none";
      }

      // Show the selected code snippet
      var selectedSnippet = document.getElementById(elementId);
      selectedSnippet.style.display = "block";
    }

    function copyToClipboard(elementId) {
      // Get the text from the code snippet
      var codeSnippet = document.getElementById(elementId);
      var text = codeSnippet.getElementsByTagName("code")[0].textContent;

      // Create a temporary textarea element for copying
      var temp = document.createElement("textarea");
      temp.value = text;
      document.body.appendChild(temp);
      temp.select();

      // Copy the text to the clipboard
      navigator.clipboard.writeText(temp.value)
        .then(() => {
            console.log('Code copied to clipboard');
            alert('Code copied to clipboard!')
        })
        .catch(err => {
            console.error('Failed to copy: ', err);
        });

      // Remove the temporary textarea
      document.body.removeChild(temp);
    }
    document.addEventListener('contextmenu', function(e) {
      e.preventDefault();
    });
  

  </script>

</body>
</html>